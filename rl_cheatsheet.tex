\documentclass[10pt]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm2e}
\usepackage{float}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex rl_cheatsheet.tex
%
% 2.
%  latex rl_cheatsheet.tex
%  dvips -P pdf  -t landscape rl_cheatsheet.dvi
%  ps2pdf rl_cheatsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
        { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
        {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
                {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
                {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        }

% Turn off header and footer
\pagestyle{empty}


% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.1ex plus .0ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.1ex plus .0ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmax}{argmax}

% -----------------------------------------------------------------------

\begin{document}

\raggedright
\small
\begin{multicols*}{2}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{Reinforcement Learning Cheat Sheet} \\
\end{center}

\section{Agent-Environment Interface}
\includegraphics[width=\linewidth]{./images/Agent-Environment.png}
The Agent at each step $t$ receives a representation of the environment's \emph{state}, $S_t \in S$ and it selects an action $A_t \in A(s)$. Then, as a consequence of its action the agent receives a \emph{reward}, $R_{t + 1} \in R \in \mathbb{R}$.

\section{Markov Decision Process}
A \textbf{Markov Decision Process}, MPD, is a 5-tuple $(S, A, P, R, \gamma)$ where:
\begin{equation*}
        \begin{array}{l}
         \text{finite set of states:} \quad s \in S \\
         \text{finite set of actions:} \quad a \in A \\
                 \text{state transition probabilities:} \\
     \quad   p(s' | s, a) = Pr \{S_{t + 1} = s' | S_t = s, A_t = a \} \quad \text{or}\\
     \quad   p(s', r | s, a) = Pr \{S_{t + 1} = s', R_{t + 1}=r | S_t = s, A_t = a \} \\
                \text{expected reward for state-action-next state:}\\
     \quad   r(s, a,s') = \mathbb{E}[ R_{t + 1} | S_{t + 1} = s',  S_t = s, A_t = a ]  
        \end{array}
\end{equation*}

\subsection{Reward}
The total \emph{reward} is expressed as $G_t = \sum_{k = 0}^H \gamma^kr_{t + k + 1}$, where $\gamma$ is the \emph{discount factor} and $H$ is the \emph{horizon}, that can be infinite.

\subsection{Policy}
A \emph{policy} is a mapping from a state to an action $\pi_t(a|s)$.
That is the probability of select an action $A_t = a$ if $S_t = s$.

%\newlength{\MyLen}
%\settowidth{\MyLen}{\texttt{letterpaper}/\texttt{a4paper} \ }




%\section{Value Function}
%Value function describes \emph{how good} is to be in a specific state $s$ under a certain policy $\pi$. For MDP:
%\begin{equation}
%V_{\pi}(s) = \mathbb{E}[G_t | S_t = s]
%\label{eq: value_func}
%\end{equation}
%
%Informally, is the expected return (expected cumulative discounted reward) when starting from $s$ and following $\pi$
%
%\subsection{Optimal}
%
%\begin{equation}
%v_*(s) = \max\limits_{\pi} v^\pi(s)
%\label{eq: value_optimal}
%\end{equation}

%\section{Action-Value (Q) Function}
%We can also denoted the expected reward for state, action pairs.
%\begin{equation}
%q_{\pi}(s, a) = \mathbb{E}_{\pi}\begin{bmatrix} G_t | S_t = s, A_t = a \end{bmatrix}
%\label{eq: q_func}
%\end{equation}
%\subsection{Optimal}
%The optimal value-action function:
%
%\begin{equation}
%q_*(s,a) = \max\limits_{\pi} q^\pi(s,a)
%\label{eq: action_value_optimal}
%\end{equation}
%Clearly, using this new notation we can redefine $v^*$, equation \eqref{eq: value_optimal}, using $q^*(s,a)$, equation \eqref{eq: action_value_optimal}:
%\begin{equation}
%v_*(s) = \max\limits_{a \in A(s)} q_{\pi*}(s,a)
%\end{equation}
%Intuitively, the above equation express the fact that the value of a state under the optimal policy \textbf{must be equal} to the expected return from the best action from that state.

\section{Bellman Equation}
%An important recursive property emerges for both Value and Q functions if we expand them.
\subsection{Value Function}
Value function describes \emph{how good} is to be in a specific state $s$ under a certain policy $\pi$.

Policy evaluation:
\begin{equation*}
\begin{array}{l l}
v_{\pi}(s) & = \mathbb{E}_{\pi}\begin{bmatrix}G_t | S_t = s\end{bmatrix} \\

& =  \mathbb{E}_{\pi}\begin{bmatrix}\sum\limits_{k = 0}^{\infty} \gamma^kR_{t + k + 1} | S_t = s \end{bmatrix} \\

& = \mathbb{E}_{\pi}\begin{bmatrix}R_{t + 1} + \gamma\sum\limits_{k = 0}^{\infty} \gamma^k R_{t + k + 2} | S_t = s \end{bmatrix} \\

& = \sum\limits_{a} \pi(a | s)\!\! \sum\limits_{s',r} p(s', r | s, a)\!\!\left[ r + \gamma \underbrace{\mathbb{E}_{\pi}\!\!\begin{bmatrix} \sum\limits_{k = 0}^{\infty} \gamma^k R_{t + k + 2} | S_{t + 1} = s' \end{bmatrix}}_{\text{Expected reward from } s_{t + 1}}  \right]\\

& = \sum\limits_{a} \pi(a | s) \sum\limits_{s',r}p(s', r | s, a)\begin{bmatrix} r + \gamma v_{\pi}(s')
\end{bmatrix}
\end{array}
%\label{eq: value_bellman}
\end{equation*}
%Similarly, we can do the same for the Q function:
\subsection{Q Function}
The expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
\begin{equation*}
\begin{array}{l l}
q_{\pi}(s,a) &= \mathbb{E}_{\pi}\begin{bmatrix} G_t | S_t = s, A_t = a \end{bmatrix} \\

&= \mathbb{E}_{\pi}\begin{bmatrix} \sum\limits_{k = 0}^{\infty}\gamma^kR_{t + k + 1} | S_t = s, A_t = a \end{bmatrix} \\

&= \mathbb{E}_{\pi}\begin{bmatrix}R_{t+1} + \gamma\sum\limits_{k = 0}^{\infty}\gamma^kR_{t + k + 2} | S_t = s, A_t = a \end{bmatrix} \\

&=\sum\limits_{s',r} p(s', r| s, a)\begin{bmatrix}r +\gamma\mathbb{E}_{\pi} \begin{bmatrix} \sum\limits_{k = 0}^{\infty} \gamma^k   R_{t + k + 2} | S_{t+1} = s' \end{bmatrix}  \end{bmatrix} \\

&=\sum\limits_{s',r} p(s', r| s, a)\begin{bmatrix}r +\gamma  v_{\pi}(s') \end{bmatrix}\\
&=\sum\limits_{s',r} p(s', r| s, a)\left[r +\gamma  \sum\limits_{a'}\pi(a'|s')q_{\pi}(s',a') \right]\\
\end{array}
%\label{eq: action_value_bellman}
\end{equation*}

\subsection{Optimal}
\begin{equation}
\begin{array}{l l}
v_*(s) &= \max\limits_{\pi} v_\pi(s) \\
&= \max\limits_{a} \sum\limits_{s'}\sum\limits_{r} p(s', r | s, a)\begin{bmatrix} r + \gamma v_{*}(s')
\end{bmatrix}
\end{array}
\label{eq: value_optimal}
\end{equation}
The optimal value-action function:

\begin{equation}
\begin{array}{l l}
q_*(s,a) &= \max\limits_{\pi} q_\pi(s,a) \\
&= \sum\limits_{s',r} p(s', r| s, a)\begin{bmatrix}r +\gamma  v_{*}(s') \end{bmatrix}
\end{array}
\label{eq: action_value_optimal}
\end{equation}
Clearly, we can redefine $v_*$, \eqref{eq: value_optimal}, using $q_*(s,a)$, \eqref{eq: action_value_optimal}:
\begin{equation*}
v_*(s) = \max\limits_{a \in A(s)} q_{*}(s,a)
\end{equation*}
Intuitively, the above equation express the fact that the value of a state under the optimal policy \textbf{must be equal} to the expected return from the best action from that state.

%\section{Contraction Mapping}
%
%\subsection{Definition}
%
%Let $(X, d)$ be a metric space and $f: X \rightarrow X$. We say that $f$ is a
%\emph{contraction} if there is a real number $k \in [0, 1)$ such that
%\begin{equation*}
%    d(f(x), f(y)) \leq k d(x, y)
%\end{equation*}
%
%for all $x$ and $y$ in $X$, where the term $k$ is called a \emph{Lipschitz coefficent} for $f$.
%
%\subsection{Contraction Mapping theorem}
%
%Let $(X,d)$ be a complete metric space and let $f: X \rightarrow X$ be a contraction.
%Then there is one and only one fixed point $x^\ast$ such that
%\begin{equation*}
% f(x^\ast) = x^\ast
%\end{equation*}
%
%Moreover, if $x$ is any point in $X$ and $f^n(x)$ is inductively defined by
%$f^2(x) = f(f(x))$, $f^3(x)=f(f^2(x))$, \ldots, $f^n(x)=f(f^{n-1}(x))$,
%then $f^n(x) \rightarrow x^\ast$ as $n \rightarrow \infty$. This theorem guarantees
%a unique optimal solution for the dynamic programming algorithms detailed below.

\section{Dynamic Programming Algorighms}
Taking advantages of the subproblem structure of the V and Q function we can find the optimal policy by just \emph{planning}
\subsection{Policy Iteration (iterative policy evaluation)}
\begin{algorithm}[H]
%\SetAlgoLined
 1. Initialisation \\
 $V(s) \in \mathbb{R}, (\text{e.g } V(s) = 0)$ and $\pi(s) \in A$  for all $s \in S$,\\
 2. Policy Evaluation \\
 \Repeat{$\Delta < \theta$ (a small positive number)}{
  $\Delta \leftarrow 0$ \\
  \ForEach{$s \in S$} {
        $v \leftarrow V(s)$\\
        $V(s) \leftarrow \sum\limits_a \pi(a|s) \sum\limits_{s', r} p(s',r | s, a) \begin{bmatrix}
                r + \gamma V(s')
        \end{bmatrix}$ \\
        $\Delta \leftarrow \max(\Delta, | v - V(s)|)$
        }

 }
 3. Policy Improvement \\
 \emph{policy-stable} $ \leftarrow $ \emph{true} \\
% \While{not policy-stable}{
  \ForEach{$s \in S$} {
        \emph{old-action} $\leftarrow \pi(s)$
        $\pi(s) \leftarrow \argmax\limits_a\sum\limits_{s', r} p(s',r | s, a)  \begin{bmatrix}
            r + \gamma V(s')
        \end{bmatrix}$ \\
        If \emph{old-action} $ \neq \pi(s)$, \emph{policy-stable} $\leftarrow$ \emph{false}
  }
  If \emph{policy-stable}, stop; else go to 2.
% }

\caption{Policy Iteration}
\end{algorithm}

\subsection{Value Iteration}
We can avoid to wait until $V(s)$ has converged and instead do policy improvement and truncated policy evaluation step in one operation
\begin{algorithm}[H]
  \SetKwInOut{Output}{ouput}
 Initialize $V(s) \in \mathbb{R}, \text{e.g } V(s) = 0$ \\
 \Repeat{$\Delta < \theta$ (a small positive number)}{
 $\Delta \leftarrow 0$ \\
  \ForEach{$s \in S$} {
        $v \leftarrow V(s)$\\
        $V(s) \leftarrow \max\limits_a \sum\limits_{s', r} p(s',r | s, a) \begin{bmatrix}
                r + \gamma V(s')
        \end{bmatrix}$ \\
        $\Delta \leftarrow \max(\Delta, | v - V(s)|)$
  }
 }
 \Output{Deterministic policy $\pi \approx \pi_*$ such that}
 $\pi(s) = \argmax\limits_a \sum\limits_{s', r} p(s',r | s, a) \begin{bmatrix}
     r + \gamma V(s')
  \end{bmatrix}$
\caption{Value Iteration}
\end{algorithm}

%\begin{tabular}{@{}ll@{}}
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\end{tabular}
%\end{multicols}
%
%\begin{multicols}{3}

\subsection{Asynchronous DP}
%Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. 
The values of states are updated in any order whatsoever, using values of other states happen to be available. Not necessarily sweep all states in one iteration.

\section{Monte Carlo Methods}
Monte Carlo (MC) is a \emph{Model Free} method, It does not require complete knowledge of the environment. It is based on \textbf{averaging sample returns} for each state-action pair. The following algorithm gives the basic implementation

\begin{algorithm}[H]
 Initialize for all $s \in S, a \in A(s):$ \\
        $\quad Q(s,a) \leftarrow \text{arbitrary}$ \\
        $\quad \pi(s) \leftarrow \text{arbitrary}$ \\
        $\quad Returns(s,a) \leftarrow \text{empty list}$ \\

 \While{forever}{
    Choose $S_0 \in S$ and $A_0 \in A(S_0)$, all pairs have probability $ > 0$ \\
    Generate an episode starting at $S_0, A_0$ following $\pi$
    \ForEach{pair $s,a$ appearing in the episode}{
        $G \leftarrow$ return following the first occurrence of $s,a$ \\
        Append $G$ to $Returns(s,a)$ \\
        $Q(s,a) \leftarrow average(Returns(s,a))$ \\
    }
    \ForEach{$s$ in the episode}{
        $\pi(s) \leftarrow \argmax\limits_a Q(s,a)$
    }
 }
\caption{On-policy Monte Carlo}
\end{algorithm}
% }
\textbf{Off-policy Monte Carlo}: use $\mu$ to generate episodes. 
\begin{equation*}
\begin{array}{l l}
V(s)&= \mathbb{E}_\pi[G_t|S_t=s]=\sum\limits_t\mathbb{E}_\pi[G_t|S_t=s] \\
&= \sum\limits_t\mathbb{E}_\mu[L_tG_t|S_t=s] \approx \frac{\sum\limits_{t\in \mathcal{J}(s)}L_tG_t}{\sum\limits_{t\in \mathcal{J}(s)}L_t},
\end{array}
%\label{eq: MC_off_policy}
\end{equation*}
where 
\begin{equation*}
L_t = \prod\limits_{k=t}\frac{\pi(a_k|s_k)}{\mu(a_k|s_k)}.
\end{equation*}

%For non-stationary problems, the Monte Carlo estimate for, e.g, $V$ is:
%\begin{equation}
%V(S_t) \leftarrow V(S_t) + \alpha \begin{bmatrix}
%        G_t - V(S_t)
%\end{bmatrix}
%\end{equation}
%Where $\alpha$ is the learning rate, how much we want to forget about past experiences.

\subsection{Rollout Algorithm}
Given $s$ and $\pi$, MC estimate $q_\pi(s,a)$. 
$\pi$ is called the \emph{rollout policy}.
The aim of rollout algorithm is to improve upon the default policy, not find an optimal policy.

\section{Temporal Difference - Q Learning}
Temporal Difference (TD) methods learn directly from raw experience without a model of the environment's dynamics. TD substitutes the expected discounted reward $G_t$ from the episode with an estimation:
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha \begin{bmatrix}
 R_{t + 1} + \gamma V(S_{t+1}) - V(S_t)
\end{bmatrix}
\end{equation}

\subsection{Sarsa (on-policy TD)}
\begin{algorithm}[H]
 Initialize $Q(s,a)$ arbitrarily and $Q(terminal, \cdot) = 0$\\
  \ForEach{episode}{
        Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
        \While{$s$ is not terminal} {
        Take action $a$, observer $r, s'$ \\
        Choose $a'$ from $s'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
                $Q(s,a) \leftarrow Q(s,a) + \alpha  \begin{bmatrix}
r +  \gamma Q(s',a') - Q(s,a)
\end{bmatrix}$\\
        $s \leftarrow s'$, $a \leftarrow a'$
        }
 }
    \caption{Sarsa}
\end{algorithm}



%The following algorithm gives a generic implementation.
\subsection{Q-learning (Off-policy TD)}
\begin{algorithm}[H]
 Initialize $Q(s,a)$ arbitrarily and $Q(terminal, \cdot) = 0$\\
  \ForEach{episode}{
        \While{$s$ is not terminal} {
        Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
        Take action $a$, observer $r, s'$ \\
        $Q(s,a) \leftarrow Q(s,a) + \alpha  \left[
r +  \gamma \max\limits_{a'}Q(s',a') - Q(s,a)\right]$\\
        $s \leftarrow s'$
        }
 }
\caption{Q-Learning}
\end{algorithm}

Dyna-Q: Indirect RL; also learns the model.
\begin{algorithm}[H]
 Initialize $Model(s,a)$ and $Q(s,a)$,  $Q(terminal, \cdot) = 0$\\
  \ForEach{episode}{
        \While{$s$ is not terminal} {
        Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
        Take action $a$, observer $r, s'$ \\
        $Q(s,a) \leftarrow Q(s,a) + \alpha  \left[
r +  \gamma \max\limits_{a'}Q(s',a') - Q(s,a)\right]$\\
$Model(s,a)\leftarrow(s',a')$, $s \leftarrow s'$\\
\Repeat{$n$ times}{
$s\leftarrow$ random previously observed sate\\
$a\leftarrow$ random action previously taken at $s$ \\
$r,s'\leftarrow Model(s,a)$\\
$Q(s,a) \leftarrow Q(s,a) + \alpha  \left[
r +  \gamma \max\limits_{a'}Q(s',a') - Q(s,a)\right]$\\
}
        }
 }
\caption{Dyna-Q}
\end{algorithm}

$Model(s,a)$ denotes the contents of the predicted next state and reward for state-action pair $(s,a)$.

\section{Approximate DP}
\subsection{TD(0)}
\begin{equation*}
w \leftarrow w + \alpha \begin{bmatrix}
 R_{t + 1} + \gamma \hat V(S_{t+1},w) - \hat V(S_t,w)
\end{bmatrix}\nabla \hat V(S_t,w)
\end{equation*}

\subsection{TD($\lambda$)}
\begin{equation*}
\begin{array}{l}
z_t=\gamma\lambda z_{t-1}+\nabla\hat V(S_t,w_t),\quad z_{-1}=0\\
\delta_t = R_{t + 1} + \gamma \hat V(S_{t+1},w_t) - \hat V(S_t,w_t)\\
w_{t+1} = w_t + \alpha_t \delta_t z_t
    \end{array}
\end{equation*}
\subsection{Derivation of TD$(1)$}
\begin{equation*}
\text{Cost}: \quad\quad J  = \frac{1}{2} \sum_{t=0}^{\infty}\left(\sum_{k=t}^{\infty}\gamma^{k-t}R_{k+1}-\hat V(s_t,w)\right)^2
\end{equation*}
Gradient descent (batched) 
\begin{equation*}
\begin{array}{l l}
w& \leftarrow w+\alpha \partial_wJ\\
&\leftarrow w+\alpha\sum_{t=0}^{\infty}\left(\sum_{k=t}^{\infty}\gamma^{k-t}R_{k+1}-\hat V(s_t,w)\right)\partial_w\hat V(s_t,w)\\
&\leftarrow w+\alpha\sum_{t=0}^{\infty}\left(\sum_{k=t}^{\infty}\gamma^{k-t}\delta_k\right)\partial_w\hat V(s_t,w)\\
&\leftarrow w+\alpha\sum_{k=0}^{\infty}\delta_k\sum_{t=0}^{k}\gamma^{k-t}\partial_w\hat V(s_t,w)
\end{array}
\end{equation*}

\subsection{Sarsa($\lambda$) and  Q($\lambda$)}

Define the $n$-step Q-Return
\begin{equation*}
    q^{(n)} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q(s_{t+n},a_{t+n})
\end{equation*}

$n$-step Sarsa update $Q$ towards the $n$-step Q-return
\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[q_t^{(n)} - Q(s_t, a_t) \right]
\end{equation*}

Forward-view Sarsa($\lambda$):
\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[q_t^\lambda - Q(s_t, a_t) \right]
\end{equation*}
\begin{equation*}
    q_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} q_t^{(n)}
\end{equation*}
Backward-view Sarsa($\lambda$): TD($\lambda$) with $\hat V\Rightarrow\hat Q$.

Backward-view Q($\lambda$): Sarsa($\lambda$) with $\hat Q(S_{t+1},a_{t+1},w_t)\Rightarrow \inf_{a'}\hat Q(S_{t+1},a',w_t)$.

%\begin{tabular}{@{}ll@{}}
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\end{tabular}

\section{Deep Q Learning}
Deep Q Learning substitutes the $Q$ function with a deep NN called \emph{Q-network}. 
It also keep track of some observation in a $memory$ in order to use them to train the network.
\begin{equation*}
L_i(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)}\begin{bmatrix}
        ( \underbrace{r + \gamma \max\limits_a Q(s',a'; \theta_{i-1})}_\text{target} - \underbrace{Q(s,a;\theta_i)}_\text{prediction})^2
\end{bmatrix}
\end{equation*}
where $\theta$ are the weights of the network and $U(D)$ is the experience replay history.

\begin{algorithm}[H]
 Initialize replay memory $D$ with capacity $N$\\
 Initialize $Q(s,a)$ arbitrarily \\
 \ForEach{episode $\in$ episodes}{
        \While{$s$ is not terminal} {
         With probability $\epsilon$ select a random action $a \in A(s)$ \\
         otherwise select $a = \max_a Q(s, a; \theta)$ \\
         Take action $a$, observer $r, s'$ \\
         Store transition $(s, a, r, s')$ in $D$ \\
         Sample random minibatch of transitions $(s_j, a_j, r_j, s'_j)$ from $D$ \\
         $\text{Set } y_j \leftarrow
           \begin{cases}
               r_j & \text{for terminal } s_j'\\
               r_j + \gamma \max\limits_a Q(s',a'; \theta) & \text{for non-terminal } s'_j
           \end{cases}$ \\
         Gradient descent on $(y_j - Q(s_j, a_j;\theta))^2$ \\
         $s \leftarrow s'$
        }
 }
\caption{Deep Q Learning}
\end{algorithm}

%\begin{tabular}{@{}ll@{}}
%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%%\\
%\end{tabular}

%\end{multicols}
%---------------------------------------------------------------------------

%\rule{0.3\linewidth}{0.25pt} \\
%Copyright \copyright\ 2018 Francesco Saverio Zuppichini

%\scriptsize
%
%\href{https://github.com/FrancescoSaverioZuppichini/Reinforcement-Learning-Cheat-Sheet}{https://github.com/FrancescoSaverioZuppichini/Reinforcement-Learning-Cheat-Sheet}
%\begin{multicols}{3}

\section{Policy gradient method}
Policy gradient (gradient ascent):
\begin{equation*}
\theta_{t+1}=\theta_t+\alpha\partial_\theta\rho(\pi_\theta)
\end{equation*}
Policy gradient theorem
\begin{equation*}
\partial_\theta\rho(\pi_\theta) \propto \sum_s\mu_s\sum_aq_\pi(s,a) \partial_\theta\pi(a|s,\theta)
\end{equation*}
$\mu_s$ is the on-policy distribution under $\pi$ (the fraction of time spent in each state).
For average reward, $\mu$ is steady state distribution.

\rule{0.3\linewidth}{0.25pt} \\
Copyright \copyright\ 2018 Tao Bian

%\begin{tabular}{@{}ll@{}}
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\end{tabular}
\end{multicols*}



\end{document}
